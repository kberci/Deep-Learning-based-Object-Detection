{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pi_simlator.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPAlAMxQO93g4p1sv68UjHS"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jKNwwObUoGvH"},"source":["## Modify Tensorflow version if needed\n"]},{"cell_type":"code","metadata":{"id":"JFcb6YCauypl","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1606751216976,"user_tz":-60,"elapsed":65199,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"560cb2e4-5cca-4128-a51c-3f0107bc8a94"},"source":["# !pip install tensorflow==2.1.0\n","# exit()\n","!pip install tf-nightly"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tf-nightly\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/c7/890964afe2c9da25067c3a2aa1f819d7087b59f0454c6a220bc51d462609/tf_nightly-2.5.0.dev20201130-cp36-cp36m-manylinux2010_x86_64.whl (398.8MB)\n","\u001b[K     |████████████████████████████████| 398.8MB 42kB/s \n","\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n","Collecting numpy~=1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n","\u001b[K     |████████████████████████████████| 14.5MB 250kB/s \n","\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.10.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.35.1)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.3.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12)\n","Collecting tf-estimator-nightly~=2.4.0.dev\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/d2/2131f5a0f0d14bae7f4d332724748b9ca6746b0d32f5c76145f0707f47d8/tf_estimator_nightly-2.4.0.dev2020102301-py2.py3-none-any.whl (461kB)\n","\u001b[K     |████████████████████████████████| 471kB 55.0MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.4.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n","Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n","Collecting tb-nightly~=2.5.0.a\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/71/774f79968c6c8bc12814ed296924e8c40d5a20b7e548f6999447981d1b03/tb_nightly-2.5.0a20201130-py3-none-any.whl (12.2MB)\n","\u001b[K     |████████████████████████████████| 12.2MB 49.6MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n","Collecting grpcio~=1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/00/b393f5d0e92b37592a41357ea3077010c95400c907f6b9af01f4f6abe140/grpcio-1.32.0-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 56.4MB/s \n","\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n","Collecting protobuf~=3.13.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/79/510974552cebff2ba04038544799450defe75e96ea5f1675dbf72cc8744f/protobuf-3.13.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 22.1MB/s \n","\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (3.3.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.17.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (0.4.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly~=2.5.0.a->tf-nightly) (50.3.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (2.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.1.1)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (4.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.5.0.a->tf-nightly) (3.0.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (1.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.5.0.a->tf-nightly) (3.4.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly~=2.5.0.a->tf-nightly) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.5.0.a->tf-nightly) (3.1.0)\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: numpy, tf-estimator-nightly, grpcio, protobuf, tb-nightly, tf-nightly\n","  Found existing installation: numpy 1.18.5\n","    Uninstalling numpy-1.18.5:\n","      Successfully uninstalled numpy-1.18.5\n","  Found existing installation: grpcio 1.33.2\n","    Uninstalling grpcio-1.33.2:\n","      Successfully uninstalled grpcio-1.33.2\n","  Found existing installation: protobuf 3.12.4\n","    Uninstalling protobuf-3.12.4:\n","      Successfully uninstalled protobuf-3.12.4\n","Successfully installed grpcio-1.32.0 numpy-1.19.4 protobuf-3.13.0 tb-nightly-2.5.0a20201130 tf-estimator-nightly-2.4.0.dev2020102301 tf-nightly-2.5.0.dev20201130\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google","numpy"]}}},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"uZdj0f-yoNMf"},"source":["\n","## Download repository"]},{"cell_type":"code","metadata":{"id":"aKDkkr9_VOQW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610418058450,"user_tz":-60,"elapsed":62287,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"dd5a0a50-8fbc-4c2c-88ae-48133fed861b"},"source":["%cd /content/\n","u = 'kberci95'\n","p = 'Kaland1995'\n","!git clone https://$u:$p@bitbucket.org/proinvent_vision/embeddeddeeplearning.git\n","%cd /content/embeddeddeeplearning/\n","!git reset --hard HEAD\n","!git pull\n","%cd /content/embeddeddeeplearning/all_models/\n","!git checkout no_tf_coral"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'embeddeddeeplearning'...\n","remote: Counting objects: 599, done.\u001b[K\n","remote: Compressing objects: 100% (587/587), done.\u001b[K\n","remote: Total 599 (delta 262), reused 0 (delta 0)\u001b[K\n","Receiving objects: 100% (599/599), 414.31 MiB | 13.17 MiB/s, done.\n","Resolving deltas: 100% (262/262), done.\n","/content/embeddeddeeplearning\n","HEAD is now at 157a051 modified config files, rerun code\n","Already up to date.\n","/content/embeddeddeeplearning/all_models\n","Branch 'no_tf_coral' set up to track remote branch 'no_tf_coral' from 'origin'.\n","Switched to a new branch 'no_tf_coral'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V-Oqp9akoX_G"},"source":["## Run the inference script"]},{"cell_type":"code","metadata":{"id":"2GuTLomHuypq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606751411223,"user_tz":-60,"elapsed":248821,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"0ab81141-db08-47a6-bff5-506014b2afd2"},"source":["# Download images:\n","!mkdir models\n","!python downloads.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Saving:  ./images/cute_dog.jpg\n","Saving:  ./images/runner_with_dog.jpg\n","Saving:  ./images/pizza_and_cake.jpg\n","Saving:  ./images/profile_picture.jpg\n","Downloading 1fK4NZAyS58OmkxQMWYda0DruHyC2O0Gf into ./models/base_models/mobilenetV2.tflite... Done.\n","Downloading 1vll8kCcMtJwJB_kH7TaycH8BF9Cj9F0a into ./models/base_models/mobiledet.tflite... Done.\n","Downloading 1VsXgpSWGJyX5YDex7fp_r0dHiJppf1Jt into ./models/base_models/efficientdetD0.tflite... Done.\n","Downloading 1dZ5HIOYLr5Kp4uSNar8eJQumjqZVl47j into ./models/base_models/yolov4-tiny.tflite... Done.\n","Downloading 1l3FcBdeTFeSn8xd5z6-mpOeNloYTAqWp into ./models/base_models/yolov5s.tflite... Done.\n","Downloading 16mE64mAGCjQWjcyDhQLSjOUBzjPLaNSm into ./models/mobilenet_models/tf1_ssdlite_v2_300x300.tflite... Done.\n","Downloading 19HRgmi5JaGlIFK8xKsersg4IN3ZX59h1 into ./models/mobilenet_models/tf1_ssd_v2_300x300.tflite... Done.\n","Downloading 1L7h8XoxV4gEJSt2AODkqXjNaH5srJQBL into ./models/mobilenet_models/tf1_ssd_v2_mnasfpn_320x320.tflite... Done.\n","Downloading 1vWC3_Ncyl1g8k5w72B5bmF9RIJCoqgyG into ./models/mobilenet_models/tf1_ssd_v2_quantized_300x300.tflite... Done.\n","Downloading 1FG1DaahXOmjJVeBfo-NZKVFchKxIQ0M6 into ./models/mobilenet_models/tf1_ssd_v3_large_320x320.tflite... Done.\n","Downloading 1E0Jch9OKURprOdmMm4C8w88bnLo1DcpG into ./models/mobilenet_models/tf1_ssd_v3_small_320x320.tflite... Done.\n","Downloading 1WtPdzt6ZSqTD62mNb-8kWVDt1xTI94pT into ./models/mobilenet_models/tf2_ssd_v2_320x320.tflite... Done.\n","Downloading 13EbOqqgQ4Xft0Tc8h1F2AbqevKwC_1Wd into ./models/mobilenet_models/tf2_ssd_v2_fpn_320x320.tflite... Done.\n","Downloading 1iOC_HWNeSvxRqX4iamGABDz2JTqsvRZC into ./models/mobilenet_models/tf2_ssd_v2_fpn_640x640.tflite... Done.\n","Downloading 1zJ2H71WJadE6W-sl4ez95lO1X1ccDYfD into ./models/edgetpu/mobilenet_int8_edgetpu.tflite... Done.\n","Downloading 1m2qnlILjIre3C7lYg-l3_vzzTNIV4vnt into ./models/edgetpu/mobiledet_int8_edgetpu.tflite... Done.\n","Downloading 1p4813g4ZUw2ZR93MB8S1mS4fj5UWjohb into ./models/edgetpu/tiny_yolov4_int8_edgetpu.tflite... Done.\n","Downloading 1BnLLHIVzFYhepBBfgKXwVtc8W-qeuSYq into ./models/edgetpu/yolov5s_int8_edgetpu.tflite... Done.\n","Downloading 1VlVSLShWoelRqBRpRpxAVVuk7yLHmGE0 into ./models/mobilenet/mobilenet_fp32.tflite... Done.\n","Downloading 18-wE6fq5Enje8FcgMnCpJ8TlaWgKAdaC into ./models/mobilenet/mobilenet_int8_fallback.tflite... Done.\n","Downloading 1zJ2H71WJadE6W-sl4ez95lO1X1ccDYfD into ./models/mobilenet/mobilenet_int8_edgetpu.tflite... Done.\n","Downloading 1ncMmJLKb_zCVeJYByR2BwuT2ZQ31swE2 into ./models/mobilenet/mobilenet_int8.tflite... Done.\n","Downloading 1sXdvza-Fe9jm4d-Mj_uvvc6fHqEQT859 into ./models/mobilenet/mobilenet_fp16.tflite... Done.\n","Downloading 1cu3f-MJjezA9EXEhjmgwxZADWwarAf61 into ./models/mobilenet/mobilenet_dr.tflite... Done.\n","Downloading 1Eq2sP-qxWt0FTA8HM9Eju65yg9wu1aWy into ./models/mobiledet/mobiledet_fp32.tflite... Done.\n","Downloading 1IS79yR11SVdeJRCP5Pgr8bxkxnX2IAqo into ./models/mobiledet/mobiledet_int8_fallback.tflite... Done.\n","Downloading 1m2qnlILjIre3C7lYg-l3_vzzTNIV4vnt into ./models/mobiledet/mobiledet_int8_edgetpu.tflite... Done.\n","Downloading 1EtzxXgFjdoG1rH1Bp0MrMoG_9CPK4HKY into ./models/mobiledet/mobiledet_int8.tflite... Done.\n","Downloading 1ZXlpi6VyEMd5wqP3DCrJgYGZzJ1haa-U into ./models/mobiledet/mobiledet_fp16.tflite... Done.\n","Downloading 1uOFjvW-Cxz6Bpr-HKD_Xk0R7PP2C5JLv into ./models/mobiledet/mobiledet_dr.tflite... Done.\n","Downloading 1FRHFMdjaTJsHuEFqKh9BslvaTcQ45LDH into ./models/yolo4/tiny_yolov4_fp32.tflite... Done.\n","Downloading 1TTjuTVARXs1oaFyND-cfu7ASFwBr27fT into ./models/yolo4/tiny_yolov4_int8_fallback.tflite... Done.\n","Downloading 1p4813g4ZUw2ZR93MB8S1mS4fj5UWjohb into ./models/yolo4/tiny_yolov4_int8_edgetpu.tflite... Done.\n","Downloading 1EJ8lbAelr-4J6QeRqc_YaGOdwyBcJgC2 into ./models/yolo4/tiny_yolov4_int8.tflite... Done.\n","Downloading 1FI2Q5nQ8VfodfRGASotM2gr2MkkU1XGu into ./models/yolo4/tiny_yolov4_fp16.tflite... Done.\n","Downloading 19k3Us6_iusQ2RpmLGakQq1B38w8RSr19 into ./models/yolo4/tiny_yolov4_dr.tflite... Done.\n","Downloading 1jzmW1Kghjz9Vwxkz3SkWGHxtq8mdHthC into ./models/yolo5/yolov5s_fp32.tflite... Done.\n","Downloading 1bplddhv76P8nK0AEOAZMebENk5wRYdiv into ./models/yolo5/yolov5s_int8_fallback.tflite... Done.\n","Downloading 1BnLLHIVzFYhepBBfgKXwVtc8W-qeuSYq into ./models/yolo5/yolov5s_int8_edgetpu.tflite... Done.\n","Downloading 1XWDIYNgYGxAlTuVD9YZmbfWyz4Kgvb1I into ./models/yolo5/yolov5s_int8.tflite... Done.\n","Downloading 12FCdRtpcYciLhqn_5OvJQIngCPGck9Gp into ./models/yolo5/yolov5s_fp16.tflite... Done.\n","Downloading 1kKx25q2AytLaocSpt0scCl_TGqqOjrYh into ./models/yolo5/yolov5s_dr.tflite... Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJkJ4R2cUHLK","executionInfo":{"status":"ok","timestamp":1606751411225,"user_tz":-60,"elapsed":247049,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"300e0635-9ea2-4917-c38e-e6006bc5e12b"},"source":["!python --version"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Python 3.6.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LodQaOSwUccv","executionInfo":{"status":"ok","timestamp":1606751414557,"user_tz":-60,"elapsed":247321,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"066fce03-13d6-4b95-9da1-78ecd65a826b"},"source":["%cd /content/\n","!wget https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl\n","!pip install tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl\n","%cd /content/embeddeddeeplearning/all_models/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n","--2020-11-30 15:50:10--  https://github.com/google-coral/pycoral/releases/download/release-frogfish/tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl\n","Resolving github.com (github.com)... 192.30.255.113\n","Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/308799580/3841a300-1f81-11eb-9440-5b4790e520d9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201130T155011Z&X-Amz-Expires=300&X-Amz-Signature=3c21d2bbc8fc8a3a634aa2653cfb3e23f83f5d0e5ef16ba63e14aa9640a02779&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=308799580&response-content-disposition=attachment%3B%20filename%3Dtflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n","--2020-11-30 15:50:11--  https://github-production-release-asset-2e65be.s3.amazonaws.com/308799580/3841a300-1f81-11eb-9440-5b4790e520d9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201130%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201130T155011Z&X-Amz-Expires=300&X-Amz-Signature=3c21d2bbc8fc8a3a634aa2653cfb3e23f83f5d0e5ef16ba63e14aa9640a02779&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=308799580&response-content-disposition=attachment%3B%20filename%3Dtflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n","Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.92.75\n","Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.92.75|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1250215 (1.2M) [application/octet-stream]\n","Saving to: ‘tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl’\n","\n","tflite_runtime-2.5. 100%[===================>]   1.19M  2.91MB/s    in 0.4s    \n","\n","2020-11-30 15:50:11 (2.91 MB/s) - ‘tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl’ saved [1250215/1250215]\n","\n","Processing ./tflite_runtime-2.5.0-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tflite-runtime==2.5.0) (1.19.4)\n","Installing collected packages: tflite-runtime\n","Successfully installed tflite-runtime-2.5.0\n","/content/embeddeddeeplearning/all_models\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YQOmKVTpWchQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606751728578,"user_tz":-60,"elapsed":2486,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"7e799ff6-5af1-4f6c-a2ec-b3a66fdc61df"},"source":["%cd /content/embeddeddeeplearning/all_models/\n","# Run inference:\n","!python inference.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/embeddeddeeplearning/all_models\n","2020-11-30 15:55:26.589454: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2020-11-30 15:55:26.589496: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","Traceback (most recent call last):\n","  File \"inference.py\", line 4, in <module>\n","    import tensorflow as tf\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 55, in <module>\n","    from ._api.v2 import compat\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/__init__.py\", line 39, in <module>\n","    from . import v1\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\", line 34, in <module>\n","    from . import compat\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\", line 39, in <module>\n","    from . import v1\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\", line 51, in <module>\n","    from tensorflow._api.v2.compat.v1 import lite\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\", line 11, in <module>\n","    from . import experimental\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\", line 10, in <module>\n","    from . import nn\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/nn/__init__.py\", line 10, in <module>\n","    from tensorflow.lite.python.lite import TFLiteLSTMCell\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py\", line 51, in <module>\n","    from tensorflow.lite.python.interpreter import Interpreter  # pylint: disable=unused-import\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py\", line 32, in <module>\n","    from tensorflow.lite.python.interpreter_wrapper import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper\n","ImportError: generic_type: type \"InterpreterWrapper\" is already registered!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dO7nMYdW-j-5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606750918018,"user_tz":-60,"elapsed":3100,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"11c3d378-00a9-411c-acd0-0ce8176d86fa"},"source":["!git config --global user.email \"k.berci95@gmail.com\"\n","!git config --global user.name \"kberci95\"\n","!git add /content/embeddeddeeplearning/all_models/results/inference_times.csv\n","!git commit -m \"colab gpu runtime results added\"\n","!git push"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[no_tf_coral 0667551] colab gpu runtime results added\n"," 1 file changed, 136 insertions(+)\n","Counting objects: 5, done.\n","Delta compression using up to 2 threads.\n","Compressing objects: 100% (5/5), done.\n","Writing objects: 100% (5/5), 2.15 KiB | 550.00 KiB/s, done.\n","Total 5 (delta 3), reused 0 (delta 0)\n","remote: \n","remote: Create pull request for no_tf_coral:\u001b[K\n","remote:   https://bitbucket.org/proinvent_vision/embeddeddeeplearning/pull-requests/new?source=no_tf_coral&t=1\u001b[K\n","remote: \n","To https://bitbucket.org/proinvent_vision/embeddeddeeplearning.git\n","   59a54c1..0667551  no_tf_coral -> no_tf_coral\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SIyum7CRGDZb"},"source":["## Visualize results"]},{"cell_type":"code","metadata":{"id":"BE4_CKSV3ymk"},"source":["import glob\n","from google.colab.patches import cv2_imshow\n","import cv2\n","\n","def show_results(path):  \n","  image_paths = glob.glob(path + '/*.jpg')\n","  f = open(path + '/inference_times.txt', 'r')\n","\n","  inference_times = {}\n","  for line in f:\n","    if 'jpg' in line:\n","      name, time = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split('.jpg')\n","      inference_times[name + '.jpg'] = time\n","  print(inference_times)\n","  for path in sorted(image_paths):\n","    img_name = path.split(\"/\")[-1]\n","    print(\"\\n\\nImage name:\\t\", img_name, \"\\nInference time:\\t\", inference_times[img_name], \"\\nNetwork type:\\t\", path.split(\"_\")[-2])\n","    img = cv2.imread(path)\n","    cv2_imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPTwKd1EGdQa"},"source":["show_results('/content/embeddeddeeplearning/mobilenet_efficientdet/results')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xe6_EzNWC_r"},"source":["# Run single image"]},{"cell_type":"code","metadata":{"id":"pcN3bczDYUKr"},"source":["import tensorflow as tf\n","\n","\n","def draw_boxes(input_details, input_image, boxes, classes, scores, confidence_threshold, model_type):\n","    # Get sizes before and after resizing\n","    _, height, width, _ = list(input_details[0]['shape'])\n","    imH, imW, _ = input_image.shape\n","\n","    # Get colors for visualization\n","    colors = get_colors(num_classes=80)\n","\n","    # Loop over all detections and draw detection box if confidence is above minimum threshold\n","    for i in range(len(scores)):\n","        if (scores[i] > confidence_threshold) and (scores[i] <= 1.0):\n","            # Get bounding box coordinates and draw box\n","            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within\n","            ymin_, xmin_, ymax_, xmax_ = boxes[i][0] * imH, boxes[i][1] * imW, boxes[i][2] * imH, boxes[i][3] * imW\n","            if model_type == 'efficientdet':\n","                ymin_ /= height\n","                xmin_ /= width\n","                ymax_ /= height\n","                xmax_ /= width\n","            ymin, xmin, ymax, xmax = int(max(1, ymin_)), int(max(1, xmin_)), int(min(imH, ymax_)), int(min(imW, xmax_))\n","\n","            # Look up object name from \"labels\" array using class index\n","            object_name, color_index = get_object_name(classes[i], model=model_type)\n","\n","            cv2.rectangle(input_image, (xmin, ymin), (xmax, ymax), colors[color_index], 2)\n","\n","            # Draw label\n","            label = '%s: %d%%' % (object_name, int(scores[i] * 100))  # Example: 'person: 72%'\n","            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)  # Get font size\n","            label_ymin = max(ymin, labelSize[1] + 10)  # Make sure not to draw label too close to top of window\n","            # Text box\n","            cv2.rectangle(input_image, (xmin, label_ymin - labelSize[1] - 10),\n","                          (xmin + labelSize[0], label_ymin + baseLine - 10), colors[color_index], cv2.FILLED)\n","            # Label text\n","            cv2.putText(input_image, label, (xmin, label_ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n","    return input_image\n","\n","def filter_boxes(box_xywh, scores, score_threshold=0.25, input_shape=tf.constant([416, 416])):\n","    scores_max = tf.math.reduce_max(scores, axis=-1)\n","    mask = scores_max >= score_threshold\n","    class_boxes = tf.boolean_mask(box_xywh, mask)\n","    pred_conf = tf.boolean_mask(scores, mask)\n","    class_boxes = tf.reshape(class_boxes, [tf.shape(scores)[0], -1, tf.shape(class_boxes)[-1]])\n","    pred_conf = tf.reshape(pred_conf, [tf.shape(scores)[0], -1, tf.shape(pred_conf)[-1]])\n","\n","    box_xy, box_wh = tf.split(class_boxes, (2, 2), axis=-1)\n","\n","    input_shape = tf.cast(input_shape, dtype=tf.float32)\n","\n","    box_yx = box_xy[..., ::-1]\n","    box_hw = box_wh[..., ::-1]\n","\n","    box_mins = (box_yx - (box_hw / 2.)) / input_shape\n","    box_maxes = (box_yx + (box_hw / 2.)) / input_shape\n","    boxes = tf.concat([\n","        box_mins[..., 0:1],  # y_min\n","        box_mins[..., 1:2],  # x_min\n","        box_maxes[..., 0:1],  # y_max\n","        box_maxes[..., 1:2]  # x_max\n","    ], axis=-1)\n","    return boxes, pred_conf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"psrRIU6J3GgH","colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"status":"error","timestamp":1603356799307,"user_tz":-120,"elapsed":899,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"5f903599-7032-454a-b4d5-62eb0be1517a"},"source":["import cv2\n","import time\n","import glob\n","import tensorflow as tf\n","import utils\n","import pandas as pd\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","import torch\n","import torchvision\n","\n","\n","PATH_TO_IMAGE = '/content/embeddeddeeplearning/all_models/images/profile_picture.jpg'\n","PATH_TO_MODEL = '/content/embeddeddeeplearning/all_models/models/yolov5s.tflite'\n","CONF_THRESHOLD = .4\n","model_type = 'yolo5'\n","\n","# Load the Tensorflow Lite model\n","interpreter = tf.lite.Interpreter(model_path=PATH_TO_MODEL)\n","interpreter.allocate_tensors()\n","# Get model details\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","print('1')\n","# Load image and resize to expected shape [1xHxWx3]\n","# Get quantization and image details\n","floating_model = input_details[0]['dtype'] == np.float32\n","_, height, width, _ = list(input_details[0]['shape'])\n","print('2')\n","# Load image and resize to expected shape [1xHxWx3]\n","image = cv2.imread(PATH_TO_IMAGE)\n","image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","image_resized = cv2.resize(image_rgb, (width, height))\n","input_data = np.expand_dims(image_resized, axis=0)\n","print('3')\n","if floating_model and model_type != 'yolo':\n","    input_mean, input_std = 127.5, 127.5\n","    input_data = (np.float32(input_data) - input_mean) / input_std\n","elif model_type == 'yolo':\n","    input_data = np.float32(input_data) / 255\n","print('4')\n","# Perform the actual detection by running the model with the image as input\n","interpreter.set_tensor(input_details[0]['index'], input_data)\n","print('5')\n","# Run the model\n","start_time = time.time()\n","interpreter.invoke()\n","inference_time = str(round(time.time() - start_time, 4))\n","print(\"\\tInference time: \" + inference_time + \" s\")\n","print('6')\n","# Retrieve detection results\n","if model_type == 'efficientdet':\n","    boxes = interpreter.get_tensor(output_details[0]['index'])[0][:, 1:5]  # Bounding box coordinates\n","    classes = interpreter.get_tensor(output_details[0]['index'])[0][:, 6]  # Class index\n","    scores = interpreter.get_tensor(output_details[0]['index'])[0][:, 5]  # Confidence\n","elif model_type == 'mobilenet':\n","    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n","    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n","    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n","\n","elif model_type == 'yolo5':\n","    output_data = interpreter.get_tensor(output_details[0]['index'])\n","    pred = torch.tensor(output_data)\n","    pred = non_max_suppression(pred, 0.4, 0.5)\n","    boxes = list(pred[0][:,0:4].numpy().astype(int))\n","    scores = list(pred[0][:,4].numpy().astype(float))\n","    classes = list(pred[0][:,5].numpy().astype(int))\n","    image_result = utils.draw_boxes(input_details, image, boxes, classes, scores, CONF_THRESHOLD, model_type)\n","print('7')\n","# Visualize result\n","cv2_imshow(image_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","\tInference time: 0.4617 s\n","6\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-b5f0423d6c63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mimage_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONF_THRESHOLD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Visualize result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/embeddeddeeplearning/all_models/utils.py\u001b[0m in \u001b[0;36mdraw_boxes\u001b[0;34m(input_details, input_image, boxes, classes, scores, confidence_threshold, model_type)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mymax_\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mxmax_\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Look up object name from \"labels\" array using class index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"QoIRVjJpdOK4"},"source":["def non_max_suppression(prediction, conf_thres=0.1, iou_thres=0.6, merge=False, classes=None, agnostic=False):\n","    \"\"\"Performs Non-Maximum Suppression (NMS) on inference results\n","\n","    Returns:\n","         detections with shape: nx6 (x1, y1, x2, y2, conf, cls)\n","    \"\"\"\n","\n","    nc = prediction[0].shape[1] - 5  # number of classes\n","    xc = prediction[..., 4] > conf_thres  # candidates\n","\n","    # Settings\n","    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n","    max_det = 300  # maximum number of detections per image\n","    time_limit = 10.0  # seconds to quit after\n","    redundant = True  # require redundant detections\n","    multi_label = nc > 1  # multiple labels per box (adds 0.5ms/img)\n","\n","    t = time.time()\n","    output = [None] * prediction.shape[0]\n","    for xi, x in enumerate(prediction):  # image index, image inference\n","        # Apply constraints\n","        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n","        x = x[xc[xi]]  # confidence\n","\n","        # If none remain process next image\n","        if not x.shape[0]:\n","            continue\n","\n","        # Compute conf\n","        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n","\n","        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n","        box = xywh2xyxy(x[:, :4])\n","\n","        # Detections matrix nx6 (xyxy, conf, cls)\n","        if multi_label:\n","            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n","            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n","        else:  # best class only\n","            conf, j = x[:, 5:].max(1, keepdim=True)\n","            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n","\n","        # Filter by class\n","        if classes:\n","            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n","\n","        # Apply finite constraint\n","        # if not torch.isfinite(x).all():\n","        #     x = x[torch.isfinite(x).all(1)]\n","\n","        # If none remain process next image\n","        n = x.shape[0]  # number of boxes\n","        if not n:\n","            continue\n","\n","        # Sort by confidence\n","        # x = x[x[:, 4].argsort(descending=True)]\n","\n","        # Batched NMS\n","        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n","        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n","        i = torch.ops.torchvision.nms(boxes, scores, iou_thres)\n","        if i.shape[0] > max_det:  # limit detections\n","            i = i[:max_det]\n","        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n","            try:  # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n","                iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n","                weights = iou * scores[None]  # box weights\n","                x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n","                if redundant:\n","                    i = i[iou.sum(1) > 1]  # require redundancy\n","            except:  # possible CUDA error https://github.com/ultralytics/yolov3/issues/1139\n","                print(x, i, x.shape, i.shape)\n","                pass\n","\n","        output[xi] = x[i]\n","        if (time.time() - t) > time_limit:\n","            break  # time limit exceeded\n","\n","    return output\n","\n","\n","def xywh2xyxy(x):\n","    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n","    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n","    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n","    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n","    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n","    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n","    return y\n","\n","def apply_classifier(x, model, img, im0):\n","    # applies a second stage classifier to yolo outputs\n","    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n","    for i, d in enumerate(x):  # per image\n","        if d is not None and len(d):\n","            d = d.clone()\n","\n","            # Reshape and pad cutouts\n","            b = xyxy2xywh(d[:, :4])  # boxes\n","            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n","            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n","            d[:, :4] = xywh2xyxy(b).long()\n","\n","            # Rescale boxes from img_size to im0 size\n","            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n","\n","            # Classes\n","            pred_cls1 = d[:, 5].long()\n","            ims = []\n","            for j, a in enumerate(d):  # per item\n","                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n","                im = cv2.resize(cutout, (224, 224))  # BGR\n","                # cv2.imwrite('test%i.jpg' % j, cutout)\n","\n","                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n","                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n","                ims.append(im)\n","\n","            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n","            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n","\n","    return x\n","\n","names = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLfg7YaNYctP","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1606749547431,"user_tz":-60,"elapsed":532,"user":{"displayName":"Bertalan Kovács","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsT6mVnQ5gCcIj04LhGiVinfQnaEwSSDOibziMdg=s64","userId":"11871629055853239586"}},"outputId":"3b48db44-a47f-4fb9-b5f1-b5fdfc546add"},"source":["tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.3.0'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"6QFkBHrDvDrO"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fzp7o-uTvFnd"},"source":["tf.compat.v1.lite.Interpreter()"],"execution_count":null,"outputs":[]}]}